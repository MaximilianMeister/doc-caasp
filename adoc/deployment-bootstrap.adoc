[[bootstrap]]
== Bootstrapping the cluster

=== Cluster Deployment

==== Cluster initialization
. Initialize the cluster on the deployed machines.
As `--control-plane` enter the IP/FQDN of your load balancer. If you do not use a load balancer use your first master node.
+
----
skuba cluster init --control-plane <LB IP/FQDN> my-cluster
----
`cluster init` generates the folder named `my-cluster` and initializes the directory that will hold the configuration for the cluster.

==== Cluster configuration

At this point you can perform some configurations before bootstrapping the cluster.

===== Prevent nodes running special workloads from being rebooted

. Open the `kured` deployment in `my-cluster/addons/kured/kured.yaml`
. Adapt the `DaemonSet` by adding one of the following flags to the `command` section of the `kured` container:
+
----
---
apiVersion: apps/v1
kind: DaemonSet
...
spec:
  ...
    ...
      ...
      containers:
        ...
          command:
            - /usr/bin/kured
            - --blocking-pod-selector=name=<NAME OF POD>
----

You can add any key/value labels to this selector:
----
--blocking-pod-selector=<LABEL KEY 1>=<LABLE VALUE 1>,<LABEL KEY 2>=<LABEL VALUE 2>
----

Alternatively you can adapt the `kured` DaemonSet also later during runtime (after bootstrap) via:
----
kubectl edit daemonset kured --namespace kube-system
----

This will restart all `kured` pods with the additional configuration flags.

==== Prevent nodes with any prometheus alerts from being rebooted

[NOTE]
====
By default, **any** prometheus alert blocks a node from reboot. However you can filter specific alerts to be ignored via the `--alert-filter-regexp` flag.
====

. Open the `kured` deployment in `my-cluster/addons/kured/kured.yaml`
. Adapt the `DaemonSet` by adding one of the following flags to the `command` section of the `kured` container:
+
----
---
apiVersion: apps/v1
kind: DaemonSet
...
spec:
  ...
    ...
      ...
      containers:
        ...
          command:
            - /usr/bin/kured
            - --prometheus-url=<PROMETHEUS SERVER URL>
            - --alert-filter-regexp=^(RebootRequired|AnotherBenignAlert|...$
----

[IMPORTANT]
====
The <PROMETHEUS SERVER URL> needs to contain the protocol (`http://` or `https://`)
====

Alternatively you can adapt the `kured` DaemonSet also later during runtime (after bootstrap) via:
----
kubectl edit daemonset kured --namespace kube-system
----

This will restart all `kured` pods with the additional configuration flags.

==== Cluster bootstrap
. Switch to the new directory.
. Now bootstrap a master node.
For `--target` enter the IP address of your first master node.
Replace `<NODE NAME>` with a unique identifier for example "master-one".
+
.Secure configuration files access
[WARNING]
====
The directory created during this step contains configuration files that allow full administrator access to your cluster. Apply best practices for access control to this folder.
====
+
----
cd my-cluster
skuba node bootstrap --user sles --sudo --target <IP/FQDN> <NODE NAME>
----
This will bootstrap the specified node as the first master in the cluster.
The process will generate authentication certificates and the `admin.conf` file that is used for authentication against the cluster.
The files will be stored in the `my-cluster` directory specified in step one.
. Add additional master nodes to the cluster.
+
Replace the `<IP/FQDN>` with the IP for the machine.
Replace `<NODE NAME>` with a unique identifier for example "master-two".
+
----
skuba node join --role master --user sles --sudo --target <IP/FQDN> <NODE NAME>
----
. Add a worker to the cluster.
+
Replace the `<IP/FQDN>` with the IP for the machine.
Replace `<NODE NAME>` with a unique identifier for example "worker-one".
+
----
skuba node join --role worker --user sles --sudo --target <IP/FQDN> <NODE NAME>
----
. Verify the nodes that you added
+
----
skuba cluster status
----
+
The output should look like this:
+
----
NAME         OS-IMAGE                              KERNEL-VERSION        CONTAINER-RUNTIME   HAS-UPDATES   HAS-DISRUPTIVE-UPDATES
master-one   SUSE Linux Enterprise Server 15 SP1   4.12.14-110-default   cri-o://1.13.3      <none>        <none>
worker-one   SUSE Linux Enterprise Server 15 SP1   4.12.14-110-default   cri-o://1.13.3      <none>        <none>
----

=== Using kubectl

You can install and use kubectl by installing the kubernetes-client package from the {productname} extension.

----
sudo zypper in kubernetes-client
----

[TIP]
====
Alternatively you can install from upstream: https://kubernetes.io/docs/tasks/tools/install-kubectl/.
====

To talk to your cluster, simply symlink the generated configuration file to `~/.kube/config`.

[source,bash]
----
ln -s ~/clusters/my-cluster/admin.conf ~/.kube/config
----

Then you can perform all cluster operations as usual. For example checking cluster status with either:

* `skuba cluster status`
+
or
* `kubectl get nodes -o wide`
+
or
* `kubectl get pods --all-namespaces`
+
[source,bash]
----
# kubectl get pods --all-namespaces

NAMESPACE     NAME                                READY     STATUS    RESTARTS   AGE
kube-system   coredns-86c58d9df4-5zftb            1/1       Running   0          2m
kube-system   coredns-86c58d9df4-fct4m            1/1       Running   0          2m
kube-system   etcd-my-master                      1/1       Running   0          1m
kube-system   kube-apiserver-my-master            1/1       Running   0          1m
kube-system   kube-controller-manager-my-master   1/1       Running   0          1m
kube-system   kube-flannel-ds-amd64-b6krs         1/1       Running   0          53s
kube-system   kube-flannel-ds-amd64-v7kt7         1/1       Running   0          2m
kube-system   kube-proxy-5qxnt                    1/1       Running   0          2m
kube-system   kube-proxy-746ws                    1/1       Running   0          53s
kube-system   kube-scheduler-my-master            1/1       Running   0          1m
----
